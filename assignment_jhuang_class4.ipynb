{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "rga"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "assignment_jhuang_class4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtZq5gKXXTvf",
        "colab_type": "text"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
        "\n",
        "**Module 4 Assignment: Classification and Regression Neural Network**\n",
        "\n",
        "**Student Name: Julia Huang**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLP-HR3wXTvj",
        "colab_type": "text"
      },
      "source": [
        "# Assignment Instructions\n",
        "\n",
        "For this assignment you will use the **crx.csv** dataset.  This is a public dataset that can be found [here](https://archive.ics.uci.edu/ml/datasets/credit+approval). You should use the CSV file on my data site, at this location: [crx.csv](https://data.heatonresearch.com/data/t81-558/crx.csv) because it includes column headers.  This is a dataset that is usually used for binary classification. There are 15 attributes, plus a target column that contains only + or -.  Some of the columns have missing values.\n",
        "\n",
        "For this assignment you will train a neural network and return the predictions.  You will submit these predictions to the **submit** function.  See [Assignment #1](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class1.ipynb) for details on how to submit an assignment or check that one was submitted.\n",
        "\n",
        "Complete the following tasks:\n",
        "\n",
        "* Your task is to replace missing values in columns *a2* and *a14* with values estimated by a neural network (one neural network for *a2* and another for *a14*).\n",
        "* Your submission file will contain the same headers as the source CSV: *a1*, *a2*, *s3*, *a4*, *a5*, *a6*, *a7*, *a8*, *a9*, *a10*, *a11*, *a12*, *a13*, *a14*, *a15*, and *a16*.\n",
        "* You should only need to modify *a2* and *a14*.\n",
        "* Neural networks can be much more powerful at filling missing variables than median and mean.\n",
        "* Train two neural networks to predict *a2* and *a14*.  \n",
        "* The y (target) for training the two nets will be *a2* and *a14*, depending on which you are trying to fill.\n",
        "* The x for training the two nets will be 's3','a8','a9','a10','a11','a12','a13','a15'.  These are chosen because it is important not to use any columns with missing values, also it could cause unwanted bias if we include the ultimate target (*a16*).\n",
        "* ONLY predict new values for missing values in *a2* and *a14*.\n",
        "* You will likely get this small warning:  Warning: The mean of column a14 differs from the solution file by 0.20238937709643778. (might not matter if small)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvzGCE5cXTvl",
        "colab_type": "text"
      },
      "source": [
        "# Assignment Submit Function\n",
        "\n",
        "You will submit the 10 programming assignments electronically.  The following submit function can be used to do this.  My server will perform a basic check of each assignment and let you know if it sees any basic problems. \n",
        "\n",
        "**It is unlikely that should need to modify this function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Jn5RnaXTvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
        "# submission counts.  The paramaters are as follows:\n",
        "# data - Pandas dataframe output.\n",
        "# key - Your student key that was emailed to you.\n",
        "# no - The assignment class number, should be 1 through 1.\n",
        "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
        "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
        "def submit(data,key,no,source_file=None):\n",
        "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
        "    if source_file is None: source_file = __file__\n",
        "    suffix = '_class{}'.format(no)\n",
        "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
        "    with open(source_file, \"rb\") as image_file:\n",
        "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
        "    ext = os.path.splitext(source_file)[-1].lower()\n",
        "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
        "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
        "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
        "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
        "    if r.status_code == 200:\n",
        "        print(\"Success: {}\".format(r.text))\n",
        "    else: print(\"Failure: {}\".format(r.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AtMGrxjXTvw",
        "colab_type": "text"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "If you are using Google CoLab, it will be necessary to mount your GDrive so that you can send your notebook during the submit process.  Running the following code will map your GDrive to /content/drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVggNYWoXTvx",
        "colab_type": "code",
        "outputId": "e4cc44ff-2d6e-4020-af01-040195c44f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iW-ZezNXTv0",
        "colab_type": "code",
        "outputId": "5609257e-6c92-497a-ea29-3c921889c853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ls /content/drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 00012.MTS\n",
            " 1162567958.jpg\n",
            "'423 Assignment1.docx.gdoc'\n",
            "'701 Welcome Letter.pdf'\n",
            "'a-d case 3.gsheet'\n",
            "'Assignment1 (1).pdf'\n",
            "'Assignment 1_ANS_2016.pdf'\n",
            " Assignment1.docx\n",
            " Assignment1.pdf\n",
            " Assignment1.pdf.gdoc\n",
            "'Assignment 2 investment.xls'\n",
            "'Assignment 2 investment.xls.gsheet'\n",
            "'Assignment 3-cloud computing.gdoc'\n",
            " CA_DMV_handbook_Chinese.pdf\n",
            "'Colab Notebooks'\n",
            "'Copy of Helicanus.gslides'\n",
            "'diagram question 2 - H Julia.pptx'\n",
            "'diagram question 2.pptx'\n",
            " Final_project_marketing.gdoc\n",
            "'GRE files.rar'\n",
            "'GRE填空机经1100题难度分级版（第一版） (1).pdf'\n",
            " GRE填空机经1100题难度分级版（第一版）.pdf\n",
            " GRE资料-LZD.zip\n",
            " HHH.gsheet\n",
            " image.jpg\n",
            "'intended v incidental.pptx'\n",
            "'marketing final ppt.gslides'\n",
            "'marketing final ppt - Line chart 1.gsheet'\n",
            "'marketing final ppt.pdf'\n",
            "'Memory music analysis.gdoc'\n",
            "'movie 2.zip'\n",
            "'MSBA Resource List.docx'\n",
            "'mus 110 exam2.gdoc'\n",
            "'music final exam .gdoc'\n",
            " MVI_9879.MOV\n",
            " MVI_9880.MOV\n",
            " MVI_9882.MOV\n",
            " MVI_9887.MOV\n",
            " MVI_9888.MOV\n",
            " MVI_9891.MOV\n",
            " MVI_9899.MOV\n",
            "'My Movie 2.mp4'\n",
            "'My Movie 3.mp4'\n",
            "'My Movie 5'\n",
            "'My Movie.mp4'\n",
            "'notice to vacate.pdf'\n",
            " Presentation1-3.gslides\n",
            " Presentation1.gslides\n",
            "'resume+cover letter.zip'\n",
            "'Revised_BBVA Case Assignment Report_Blank.gsheet'\n",
            "'Session 3.gdoc'\n",
            "'Session 3.pdf'\n",
            "'Spring 2018 Part 1 Answer Sheet.docx'\n",
            "'Team essay.gdoc'\n",
            "'To-do list.gsheet'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled spreadsheet.gsheet'\n",
            "'巍哥GRE数学170难题2.0 .pdf'\n",
            " 巍哥数学机经200题.pdf\n",
            " 演讲稿.pages.zip\n",
            " 申请进程表-黄诗淇.xls\n",
            " 考满分GRE填空1100机经题答案.txt\n",
            " 考满分GRE阅读机经200篇答案.pdf\n",
            " 考满分GRE阅读机经200篇答案.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "giQNO-oEXTv4",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #4 Sample Code\n",
        "\n",
        "The following code provides a starting point for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws5eFYpyXTv6",
        "colab_type": "code",
        "outputId": "c6b14556-6442-47f7-e85a-4d0f5bee5405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# This is your student key that I emailed to you at the beginnning of the semester.\n",
        "key = \"Yg3Uc8sn118A6HaWAFSKG5g1Th1nOyw34jLD5Uh8\"  # This is an example key and will not work.\n",
        "\n",
        "# You must also identify your source file.  (modify for your local setup)\n",
        "# file='/resources/t81_558_deep_learning/assignment_yourname_class1.ipynb'  # IBM Data Science Workbench\n",
        "# file='C:\\\\Users\\\\jeffh\\\\projects\\\\t81_558_deep_learning\\\\t81_558_class1_intro_python.ipynb'  # Windows\n",
        "#file='/Users/jheaton/projects/t81_558_deep_learning/assignments/assignment_hjulia_class4.ipynb'  # Mac/Linux\n",
        "file='/content/drive/My Drive/Colab Notebooks/assignment_jhuang_class4.ipynb' \n",
        "#file = \"C:\\\\Users\\\\jeffh\\\\Dropbox\\\\school\\\\teaching\\\\wustl\\\\classes\\\\T81_558_deep_learning\\\\solutions\\\\assignment_solution_class4.ipynb\"\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "\n",
        "# Begin assignment\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\",na_values=['?'])\n",
        "\n",
        "\n",
        "def fill_missing_numeric(df,current,target):\n",
        "    df_original = df.copy(deep=True)\n",
        "    \n",
        "    print(df[['s3', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a15']].head())\n",
        "    df['a9'] = pd.factorize(df.a9)[0]\n",
        "    df['a10'] = pd.factorize(df.a10)[0]\n",
        "    df['a12'] = pd.factorize(df.a12)[0]\n",
        "    df['a13'] = pd.factorize(df.a13)[0]\n",
        "  \n",
        "    x = df[['s3', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a15']].values\n",
        "    y = df[current].values\n",
        "    \n",
        "    print(df[['s3', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a15']].head())\n",
        "          \n",
        "    test_idx = df[current].isnull()\n",
        "    train_idx = [not a for a in df[current].isnull().values]\n",
        "#     df_test = df[is_null]\n",
        "#     df_train = df[is_not_null]\n",
        "#     print(df_train)\n",
        "    x_train, y_train = x[train_idx], y[train_idx]\n",
        "  \n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(    \n",
        "      x_train, y_train, test_size=0.25, random_state=42)\n",
        "  \n",
        "    x_test, y_test = x[test_idx], y[test_idx]\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "    model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "    model.add(Dense(1)) # Output\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n",
        "        restore_best_weights=True)\n",
        "    model.fit(x_train,y_train,validation_data=(x_valid,y_valid),callbacks=[monitor],verbose=2,epochs=5)\n",
        "    \n",
        "    preds = model.predict(x_test)\n",
        "    preds = preds.reshape(1, -1)[0]\n",
        "#     print(preds)\n",
        "#     print(len(df[test_idx]), len(preds))\n",
        "#     df[test_idx][current] = np.array(preds)\n",
        "    \n",
        "    print(df_original.head())\n",
        "    for idx, pred in zip(df.index[test_idx].values, preds):\n",
        "      print(idx, pred)\n",
        "      df_original.iloc[idx].loc[current] = pred\n",
        "    print(df_original.head())\n",
        "    return df_original\n",
        "\n",
        "df_submit = fill_missing_numeric(df,'a2','a16')\n",
        "df_submit = fill_missing_numeric(df,'a14','a16')\n",
        "\n",
        "\n",
        "submit(source_file=file,data=df_submit,key=key,no=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      s3    a8 a9 a10  a11 a12 a13  a15\n",
            "0  0.000  1.25  t   t    1   f   g    0\n",
            "1  4.460  3.04  t   t    6   f   g  560\n",
            "2  0.500  1.50  t   f    0   f   g  824\n",
            "3  1.540  3.75  t   t    5   t   g    3\n",
            "4  5.625  1.71  t   f    0   f   s    0\n",
            "      s3    a8  a9  a10  a11  a12  a13  a15\n",
            "0  0.000  1.25   0    0    1    0    0    0\n",
            "1  4.460  3.04   0    0    6    0    0  560\n",
            "2  0.500  1.50   0    1    0    0    0  824\n",
            "3  1.540  3.75   0    0    5    1    0    3\n",
            "4  5.625  1.71   0    1    0    0    1    0\n",
            "Train on 508 samples, validate on 170 samples\n",
            "Epoch 1/5\n",
            "508/508 - 0s - loss: 1593.6172 - val_loss: 3003.7991\n",
            "Epoch 2/5\n",
            "508/508 - 0s - loss: 1140.8766 - val_loss: 13672.1313\n",
            "Epoch 3/5\n",
            "508/508 - 0s - loss: 1352.7570 - val_loss: 1057.5915\n",
            "Epoch 4/5\n",
            "508/508 - 0s - loss: 1027.7766 - val_loss: 7109.3938\n",
            "Epoch 5/5\n",
            "508/508 - 0s - loss: 931.0018 - val_loss: 1048.6843\n",
            "  a1     a2     s3 a4 a5 a6 a7    a8 a9 a10  a11 a12 a13    a14  a15 a16\n",
            "0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  202.0    0   +\n",
            "1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g   43.0  560   +\n",
            "2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  280.0  824   +\n",
            "3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  100.0    3   +\n",
            "4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  120.0    0   +\n",
            "83 3.753768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "86 1.0957069\n",
            "92 6.0262036\n",
            "97 1.1450014\n",
            "254 4.4816504\n",
            "286 0.9279249\n",
            "329 3.5648246\n",
            "445 10.201503\n",
            "450 4.7119546\n",
            "500 6.13141\n",
            "515 9.244883\n",
            "608 3.0759432\n",
            "  a1     a2     s3 a4 a5 a6 a7    a8 a9 a10  a11 a12 a13    a14  a15 a16\n",
            "0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  202.0    0   +\n",
            "1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g   43.0  560   +\n",
            "2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  280.0  824   +\n",
            "3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  100.0    3   +\n",
            "4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  120.0    0   +\n",
            "      s3    a8  a9  a10  a11  a12  a13  a15\n",
            "0  0.000  1.25   0    0    1    0    0    0\n",
            "1  4.460  3.04   0    0    6    0    0  560\n",
            "2  0.500  1.50   0    1    0    0    0  824\n",
            "3  1.540  3.75   0    0    5    1    0    3\n",
            "4  5.625  1.71   0    1    0    0    1    0\n",
            "      s3    a8  a9  a10  a11  a12  a13  a15\n",
            "0  0.000  1.25   0    0    1    0    0    0\n",
            "1  4.460  3.04   0    0    6    0    0  560\n",
            "2  0.500  1.50   0    1    0    0    0  824\n",
            "3  1.540  3.75   0    0    5    1    0    3\n",
            "4  5.625  1.71   0    1    0    0    1    0\n",
            "Train on 507 samples, validate on 170 samples\n",
            "Epoch 1/5\n",
            "507/507 - 1s - loss: 115016.6197 - val_loss: 1406700.6904\n",
            "Epoch 2/5\n",
            "507/507 - 0s - loss: 85462.5632 - val_loss: 719574.4482\n",
            "Epoch 3/5\n",
            "507/507 - 0s - loss: 65416.9972 - val_loss: 381179.9310\n",
            "Epoch 4/5\n",
            "507/507 - 0s - loss: 57220.4570 - val_loss: 245730.1108\n",
            "Epoch 5/5\n",
            "507/507 - 0s - loss: 54498.7158 - val_loss: 151160.3096\n",
            "  a1     a2     s3 a4 a5 a6 a7    a8  a9  a10  a11  a12  a13    a14  a15 a16\n",
            "0  b  30.83  0.000  u  g  w  v  1.25   0    0    1    0    0  202.0    0   +\n",
            "1  a  58.67  4.460  u  g  q  h  3.04   0    0    6    0    0   43.0  560   +\n",
            "2  a  24.50  0.500  u  g  q  h  1.50   0    1    0    0    0  280.0  824   +\n",
            "3  b  27.83  1.540  u  g  w  v  3.75   0    0    5    1    0  100.0    3   +\n",
            "4  b  20.17  5.625  u  g  w  v  1.71   0    1    0    0    1  120.0    0   +\n",
            "71 9.171769\n",
            "202 20.543358\n",
            "206 3.829759\n",
            "243 898.69965\n",
            "270 3.829759\n",
            "278 13.629071\n",
            "330 3.829759\n",
            "406 2.7675884\n",
            "445 175.08894\n",
            "456 3.829759\n",
            "592 3.829759\n",
            "622 3.829759\n",
            "626 8.620825\n",
            "  a1     a2     s3 a4 a5 a6 a7    a8  a9  a10  a11  a12  a13    a14  a15 a16\n",
            "0  b  30.83  0.000  u  g  w  v  1.25   0    0    1    0    0  202.0    0   +\n",
            "1  a  58.67  4.460  u  g  q  h  3.04   0    0    6    0    0   43.0  560   +\n",
            "2  a  24.50  0.500  u  g  q  h  1.50   0    1    0    0    0  280.0  824   +\n",
            "3  b  27.83  1.540  u  g  w  v  3.75   0    0    5    1    0  100.0    3   +\n",
            "4  b  20.17  5.625  u  g  w  v  1.71   0    1    0    0    1  120.0    0   +\n",
            "Success: Submitted Assignment #4 for hjulia:\n",
            "This is your first submission of this assignment.\n",
            "No warnings on your data. You will probably do well, but no guarantee. :-)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht8IoZmqdov3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}