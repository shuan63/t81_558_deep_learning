{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "rga"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "assignment_jhuang_class5.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq20aXMe0l5P",
        "colab_type": "text"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), School of Engineering and Applied Science, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).\n",
        "\n",
        "**Module 5 Assignment: K-Fold Cross-Validation**\n",
        "\n",
        "**Student Name: Julia Huang**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDYDUDWz0l5R",
        "colab_type": "text"
      },
      "source": [
        "# Assignment Instructions\n",
        "\n",
        "For this assignment you will use the **reg-33-data.csv** dataset.  This is a dataset that I generated specifically for this semester.  You can find the CSV file on my data site, at this location: [reg-33-data.csv](https://data.heatonresearch.com/data/t81-558/datasets/reg-33-data.csv).\n",
        "\n",
        "You will train 5 neural networks, one for each fold of a 5-fold cross validation and return the out of sample predictions.  You will submit these predictions to the **submit** function.  See [Assignment #1](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class1.ipynb) for details on how to submit an assignment or check that one was submitted.\n",
        "\n",
        "Complete the following tasks:\n",
        "\n",
        "* Normalize all numerics to zscores and all text/categoricals to dummies.  Do not normalize the *target*.\n",
        "* Your target (y) is the field named *target*.\n",
        "* If you find any missing values (NA's), replace them with the median values for that column.\n",
        "* Use a 5-fold cross validation and return out of sample predictions.  Your RMSE will not be as good as assignment #4, but this is because #4 was overfit.\n",
        "* Your submission should contain the id (column name *id*), your prediction (column name *pred\"), the expected value (from the **reg-33-data.csv** dataset, named *y*, and the absolute value of the difference between the expected and predicted (column name *diff*).\n",
        "* You might get warnings about the means of your columns differing from mine.  Do not worry about small differences. My RMSE was around 9,000. There is a large range in y, so the RMSE will be higher on this data set.\n",
        "* Your submitted dataframe will have these columns: id, y, pred, diff.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOf51Mx20l5S",
        "colab_type": "text"
      },
      "source": [
        "# Assignment Submit Function\n",
        "\n",
        "You will submit the 10 programming assignments electronically.  The following submit function can be used to do this.  My server will perform a basic check of each assignment and let you know if it sees any basic problems. \n",
        "\n",
        "**It is unlikely that should need to modify this function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX_vQfh20l5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
        "# submission counts.  The paramaters are as follows:\n",
        "# data - Pandas dataframe output.\n",
        "# key - Your student key that was emailed to you.\n",
        "# no - The assignment class number, should be 1 through 1.\n",
        "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
        "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
        "def submit(data,key,no,source_file=None):\n",
        "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
        "    if source_file is None: source_file = __file__\n",
        "    suffix = '_class{}'.format(no)\n",
        "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
        "    with open(source_file, \"rb\") as image_file:\n",
        "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
        "    ext = os.path.splitext(source_file)[-1].lower()\n",
        "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
        "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
        "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
        "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
        "    if r.status_code == 200:\n",
        "        print(\"Success: {}\".format(r.text))\n",
        "    else: print(\"Failure: {}\".format(r.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPj7_S10l5a",
        "colab_type": "text"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "If you are using Google CoLab, it will be necessary to mount your GDrive so that you can send your notebook during the submit process.  Running the following code will map your GDrive to /content/drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl7V3eM80l5c",
        "colab_type": "code",
        "outputId": "48b5f4eb-bbb9-4b80-cb9f-7c9c5d32ae6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuTwKqhj0l5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /content/drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0KZEQOFj0l5q",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #5 Sample Code\n",
        "\n",
        "The following code provides a starting point for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI4gjHgqwKo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daL7agbLwNDn",
        "colab_type": "text"
      },
      "source": [
        "### data_preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QGrXV_Mc0l5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Below is just a suggestion of how to begin.  \n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# This is your student key that I emailed to you at the beginnning of the semester.\n",
        "key = \"Yg3Uc8sn118A6HaWAFSKG5g1Th1nOyw34jLD5Uh8\"  # This is an example key and will not work.\n",
        "\n",
        "# You must also identify your source file.  (modify for your local setup)\n",
        "# file='/resources/t81_558_deep_learning/assignment_yourname_class1.ipynb'  # IBM Data Science Workbench\n",
        "# file='C:\\\\Users\\\\jeffh\\\\projects\\\\t81_558_deep_learning\\\\t81_558_class1_intro_python.ipynb'  # Windows\n",
        "# file='/Users/jheaton/projects/t81_558_deep_learning/assignments/assignment_yourname_class5.ipynb'  # Mac/Linux\n",
        "# file = \"C:\\\\Users\\\\jeffh\\\\Dropbox\\\\school\\\\teaching\\\\wustl\\\\classes\\\\T81_558_deep_learning\\\\solutions\\\\assignment_solution_class5.ipynb\"\n",
        "file='/content/drive/My Drive/Colab Notebooks/assignment_jhuang_class5.ipynb' \n",
        "\n",
        "# Begin assignment\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/datasets/reg-33-data.csv\")\n",
        "\n",
        "\n",
        "# Encode the feature vector\n",
        "ids = df['id']\n",
        "df.drop('id',1,inplace=True)\n",
        "\n",
        "# Generate dummies for convention\n",
        "df = pd.concat([df,pd.get_dummies(df['convention'],prefix=\"convention\")],axis=1)\n",
        "df.drop('convention', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for cat2\n",
        "df = pd.concat([df,pd.get_dummies(df['cat2'],prefix=\"cat2\")],axis=1)\n",
        "df.drop('cat2', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for usage\n",
        "df = pd.concat([df,pd.get_dummies(df['usage'],prefix=\"usage\")],axis=1)\n",
        "df.drop('usage', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for region\n",
        "df = pd.concat([df,pd.get_dummies(df['region'],prefix=\"region\")],axis=1)\n",
        "df.drop('region', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for code\n",
        "df = pd.concat([df,pd.get_dummies(df['code'],prefix=\"code\")],axis=1)\n",
        "df.drop('code', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for item\n",
        "df = pd.concat([df,pd.get_dummies(df['item'],prefix=\"item\")],axis=1)\n",
        "df.drop('item', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for usage\n",
        "df = pd.concat([df,pd.get_dummies(df['country'],prefix=\"country\")],axis=1)\n",
        "df.drop('country', axis=1, inplace=True)\n",
        "\n",
        "# Missing values for height\n",
        "med = df['height'].median()\n",
        "df['height'] = df['height'].fillna(med)\n",
        "\n",
        "# Missing values for length\n",
        "med = df['length'].median()\n",
        "df['length'] = df['length'].fillna(med)\n",
        "\n",
        "# Standardize ranges\n",
        "df['height'] = zscore(df['height'])\n",
        "df['max'] = zscore(df['max'])\n",
        "df['number'] = zscore(df['number'])\n",
        "df['length'] = zscore(df['length'])\n",
        "df['power'] = zscore(df['power'])\n",
        "df['weight'] = zscore(df['weight'])\n",
        "# df['target'] = zscore(df['target'])\n",
        "\n",
        "# Convert to numpy - Classification\n",
        "x_columns = df.columns.drop('target')\n",
        "x = df[x_columns].values\n",
        "# y = df['current'].values\n",
        "y = df['target'].values\n",
        "\n",
        "x_temp = x.copy()\n",
        "y_temp = y.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgR_RHWE4hWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IP51cHdwSLy",
        "colab_type": "text"
      },
      "source": [
        "### Cross_validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltK2vcx-v4pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08R851Kavq73",
        "colab_type": "code",
        "outputId": "f7f5451f-c16f-4210-8798-a870ff7c2bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10809, 256)\n",
            "(10809,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoUdyCKn0l5v",
        "colab_type": "code",
        "outputId": "956f121f-8e2c-48ba-c1ea-06e0fd22df71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "\n",
        "# Cross-Validate\n",
        "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
        "    \n",
        "oss_y = []\n",
        "oss_pred = []\n",
        "\n",
        "fold = 0\n",
        "for train, test in kf.split(x):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "          \n",
        "    x_train = x[train]\n",
        "    y_train = y[train]\n",
        "    x_test = x[test]\n",
        "    y_test = y[test]\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, input_dim=x.shape[1], activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    \n",
        "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=0,epochs=500)\n",
        "    \n",
        "    pred = model.predict(x_test)\n",
        "    \n",
        "    oss_y.append(y_test)\n",
        "    oss_pred.append(pred)    \n",
        "      \n",
        "    # Measure this fold's RMSE\n",
        "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "    print(f\"Fold score (RMSE): {score}\")\n",
        "  \n",
        "# Build the prediction list and calculate the error.\n",
        "oss_y = np.concatenate(oss_y)\n",
        "oss_pred = np.concatenate(oss_pred)\n",
        "diff = np.absolute(oss_pred - oss_y)\n",
        "score = np.sqrt(metrics.mean_squared_error(oss_pred,oss_y))\n",
        "print(f\"Final, out of sample score (RMSE): {score}\")    \n",
        "\n",
        "# Write the cross-validated prediction\n",
        "oss_y = pd.DataFrame(oss_y)\n",
        "oss_pred = pd.DataFrame(oss_pred)\n",
        "diff = pd.DataFrame(diff)\n",
        "ossDF = pd.concat( [df, oss_y, oss_pred, diff],axis=1 )\n",
        "# ossDF.to_csv(ossDF1_write,index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold #1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Fold score (RMSE): 1380.3402588951717\n",
            "Fold #2\n",
            "Fold score (RMSE): 1056.7969327369344\n",
            "Fold #3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0215a8339963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WVhXDYU-lLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oss_y.rename(columns={0: 'y'}, inplace=True)\n",
        "oss_pred.rename(columns={0: 'pred'}, inplace=True)\n",
        "# pd.concat([oss_y, oss_pred], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXwl-XWu9GDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = pd.concat([oss_y,oss_pred],axis=1)\n",
        "map_y_to_target={temp.loc[x, 'y']: temp.loc[x, 'pred'] for x in range(len(temp))}   #####\n",
        "# print(pd.concat([df['target'], oss_y, oss_pred], axis=1)\n",
        "temp = pd.concat([df[['target']], oss_y], axis=1)\n",
        "temp = pd.concat([ids, temp], axis=1)\n",
        "temp['y'] = temp['y'].map(map_y_to_target)   ###### map\n",
        "temp.rename(columns={'y': 'pred'}, inplace=True)\n",
        "temp.rename(columns={'target': 'y'}, inplace=True)\n",
        "temp['diff'] = abs(temp['y'] - temp['pred'])\n",
        "temp.head()\n",
        "\n",
        "df_submit = temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVdVJZq5YjG",
        "colab_type": "code",
        "outputId": "1d3c8a0b-6a18-4f7c-e771-cd91a2cd8a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "submit(source_file=file,data=df_submit,key=key,no=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success: Submitted Assignment #5 for hjulia:\n",
            "This is your first submission of this assignment.\n",
            "No warnings on your data. You will probably do well, but no guarantee. :-)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANLVb_TUy2OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsXYoNhx0l5y",
        "colab_type": "code",
        "outputId": "e83aab38-b4bc-4981-9dde-7f9b91a77bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8648, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNjYj1d6zKSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}